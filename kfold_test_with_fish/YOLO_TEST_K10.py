from ultralytics import YOLO
import os

# --- 1. ê¸°ë³¸ ì„¤ì • ---
K = 10  # K-Fold íšŸìˆ˜

# 10ê°œì˜ 'í•™ìŠµëœ ëª¨ë¸'ì´ ìžˆëŠ” í´ë”
MODELS_BASE_DIR = 'models_k10_runs_70per' 
# 10ê°œì˜ 'OODê°€ í¬í•¨ëœ ë°ì´í„°ì…‹'ì´ ìžˆëŠ” í´ë”
DATA_BASE_DIR = 'A_k10_runs_70per'  

# --- 2. í‰ê°€/ì˜ˆì¸¡ ê²°ê³¼ ì €ìž¥ ìœ„ì¹˜ ì„¤ì • ---

# 1. model.val()ì˜ ì‹œê°í™” ê²°ê³¼(PR curve ë“±)ê°€ ì €ìž¥ë  í”„ë¡œì íŠ¸
EVAL_PROJECT = 'result_k_10_70per'
# 2. model.predict()ì˜ ì˜ˆì¸¡ ì´ë¯¸ì§€ê°€ ì €ìž¥ë  í”„ë¡œì íŠ¸
PREDICT_PROJECT = 'predict_imgs_k_10_70per'
# 3. 10ê°œ Runì˜ 'metrics.txt' ìš”ì•½ íŒŒì¼ì´ ì €ìž¥ë  í´ë”
METRICS_SUMMARY_DIR = 'k10_metrics_summary_70per' 

os.makedirs(METRICS_SUMMARY_DIR, exist_ok=True)
all_metrics_list = [] # 10ê°œ Runì˜ ì§€í‘œë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸

print(f"--- K={K} Fold Evaluation & Prediction Start ---")
print(f"Data source: {DATA_BASE_DIR}")
print(f"Models source: {MODELS_BASE_DIR}")

# --- 3. K=10 (1~10) ë£¨í”„ ì‹¤í–‰ ---
for i in range(1, K + 1):
    run_name = f'run_{i}_test'
    
    print("\n" + "="*60)
    print(f"ðŸš€ [Run {i}/{K}] STARTING: {run_name}")
    print("="*60 + "\n")

    # --- 4. ì´ Runì— í•„ìš”í•œ ê²½ë¡œ ì •ì˜ ---
    model_path = os.path.join(MODELS_BASE_DIR, run_name, 'weights', 'best.pt')
    data_yaml_path = os.path.join(DATA_BASE_DIR, run_name, 'data.yaml')
    predict_source_path = os.path.join(DATA_BASE_DIR, run_name, 'test', 'images')

    # --- 5. íŒŒì¼ ì¡´ìž¬ ì—¬ë¶€ í™•ì¸ ---
    if not os.path.exists(model_path):
        print(f"âš ï¸ 'best.pt' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
    if not os.path.exists(data_yaml_path):
        print(f"âš ï¸ 'data.yaml' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_yaml_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
    if not os.path.exists(predict_source_path):
        print(f"âš ï¸ 'test/images' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {predict_source_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
        
    # --- 6. ëª¨ë¸ ë¡œë“œ ---
    try:
        model = YOLO(model_path)
        model.to('cpu') # ìš”ì²­í•˜ì‹  ëŒ€ë¡œ CPUë¡œ ì„¤ì •
        print(f"âœ… Model loaded: {model_path}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    #####################
    # 7. ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰ #
    #####################
    print(f"  [1/2] Evaluating 'test' split...")
    try:
        metrics = model.val(
            data=data_yaml_path,
            split='test',        # âœ… 'test' ìŠ¤í”Œë¦¿ì„ ì‚¬ìš©í•˜ë„ë¡ ëª…ì‹œ
            imgsz=416,
            batch=16,
            project=EVAL_PROJECT,
            name=run_name,
            exist_ok=True,
            device='cpu'         # CPUì—ì„œ í‰ê°€ ìˆ˜í–‰
        )

        # í‰ê°€ ì§€í‘œ ì¶”ì¶œ
        map50 = metrics.box.map50
        map_all = metrics.box.map
        precision = metrics.box.mp
        recall = metrics.box.mr
        
        # ìµœì¢… ìš”ì•½ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
        all_metrics_list.append({
            'run': run_name,
            'map50': map50,
            'map': map_all,
            'precision': precision,
            'recall': recall
        })

        # í‰ê°€ ì§€í‘œ ì¶œë ¥
        print(f"    ðŸ“Š [Metrics for {run_name}]")
        print(f"    mAP@0.5:          {map50:.4f}")
        print(f"    mAP@0.5:0.95:     {map_all:.4f}")

        # ê°œë³„ í‰ê°€ ì§€í‘œ íŒŒì¼ ì €ìž¥ (ìš”ì•½ í´ë”ì—)
        output_dir = os.path.join(METRICS_SUMMARY_DIR, run_name)
        os.makedirs(output_dir, exist_ok=True)
        metrics_path = os.path.join(output_dir, 'metrics.txt')

        with open(metrics_path, 'w') as f:
            f.write(f"[ëª¨ë¸ í‰ê°€ ì§€í‘œ: {run_name}]\n")
            f.write(f"mAP@0.5:          {map50:.4f}\n")
            f.write(f"mAP@0.5:0.95:     {map_all:.4f}\n")
            f.write(f"Precision (mean): {precision:.4f}\n")
            f.write(f"Recall (mean):    {recall:.4f}\n")
        
        print(f"    -> ê°œë³„ metrics.txt ì €ìž¥ ì™„ë£Œ: {metrics_path}")

    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}. ì˜ˆì¸¡ ë‹¨ê³„ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")

    #######################################
    # 8. ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€ ì €ìž¥ (ì‹œê°í™”ìš©) #
    #######################################
    print(f"  [2/2] Predicting on 'test/images'...")
    try:
        results = model.predict(
            source=predict_source_path,
            imgsz=416,
            save=True,
            project=PREDICT_PROJECT,
            name=run_name,
            exist_ok=True,
            batch=16,
            device='cpu'
        )
        print(f"    -> ì˜ˆì¸¡ ì´ë¯¸ì§€ ì €ìž¥ ì™„ë£Œ (Project: {PREDICT_PROJECT})")
    
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    print(f"âœ… [Run {i}/{K}] FINISHED: {run_name}")


# --- 9. [ìµœì¢… ìš”ì•½] 10ê°œ Runì˜ í‰ê·  mAP ê³„ì‚° ---
print("\n" + "="*60)
print("ðŸŽ‰ All 10 K-Fold evaluations are complete.")
print(f"ðŸ“Š [K-Fold ìµœì¢… ìš”ì•½ (from {DATA_BASE_DIR})]")
print("="*60 + "\n")

if not all_metrics_list:
    print("âŒ ê³„ì‚°ëœ metricsê°€ ì—†ìŠµë‹ˆë‹¤. ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
else:
    # mAP@0.5 ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
    all_metrics_list.sort(key=lambda x: x['map50'], reverse=True)
    
    summary_file_path = os.path.join(METRICS_SUMMARY_DIR, '_K10_Final_Summary.txt')
    
    with open(summary_file_path, 'w') as f:
        f.write(f"[K-Fold ìµœì¢… ìš”ì•½: {DATA_BASE_DIR}]\n\n")
        f.write("--- ê°œë³„ Run ì„±ëŠ¥ (mAP@0.5 ê¸°ì¤€ ì •ë ¬) ---\n")
        
        for metrics in all_metrics_list:
            line = f"  {metrics['run']}: mAP@0.5={metrics['map50']:.4f}, mAP@.5:.95={metrics['map']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\n"
            print(line, end='')
            f.write(line)

        # í‰ê·  ê³„ì‚°
        avg_map50 = sum(m['map50'] for m in all_metrics_list) / len(all_metrics_list)
        avg_map = sum(m['map'] for m in all_metrics_list) / len(all_metrics_list)
        avg_p = sum(m['precision'] for m in all_metrics_list) / len(all_metrics_list)
        avg_r = sum(m['recall'] for m in all_metrics_list) / len(all_metrics_list)
        
        print("-------------------------------------------------")
        print(f"  ðŸ”¥ AVERAGE mAP@0.5:      {avg_map50:.4f}")
        print(f"  ðŸ”¥ AVERAGE mAP@.5:.95: {avg_map:.4f}")
        print(f"  ðŸ”¥ AVERAGE Precision:  {avg_p:.4f}")
        print(f"  ðŸ”¥ AVERAGE Recall:     {avg_r:.4f}")

        f.write("\n--- K-Fold í‰ê·  (N=10) ---\n")
        f.write(f"AVERAGE mAP@0.5:      {avg_map50:.4f}\n")
        f.write(f"AVERAGE mAP@.5:.95: {avg_map:.4f}\n")
        f.write(f"AVERAGE Precision:  {avg_p:.4f}\n")
        f.write(f"AVERAGE Recall:     {avg_r:.4f}\n")
        
    print(f"\nâœ… K-Fold ìµœì¢… ìš”ì•½ íŒŒì¼ ì €ìž¥ ì™„ë£Œ!\n   -> {summary_file_path}")