from ultralytics import YOLO
import os
import numpy as np                # ğŸ‘ˆ [ì¶”ê°€ë¨] í‰ê·  ê³„ì‚°ì„ ìœ„í•´ numpy ì„í¬íŠ¸
import matplotlib.pyplot as plt   # ğŸ‘ˆ [ì¶”ê°€ë¨] ê·¸ë˜í”„ ìƒì„±ì„ ìœ„í•´ matplotlib ì„í¬íŠ¸

# --- 1. ê¸°ë³¸ ì„¤ì • ---
K = 10  # K-Fold íšŸìˆ˜

# 10ê°œì˜ 'í•™ìŠµëœ ëª¨ë¸'ì´ ìˆëŠ” í´ë”
MODELS_BASE_DIR = 'models_k10_runs_70per' 
# 10ê°œì˜ 'OODê°€ í¬í•¨ëœ ë°ì´í„°ì…‹'ì´ ìˆëŠ” í´ë”
DATA_BASE_DIR = 'A_k10_runs_70per'  

# --- 2. í‰ê°€/ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜ ì„¤ì • ---
EVAL_PROJECT = 'result_k_10_70per_2'
PREDICT_PROJECT = 'predict_imgs_k_10_70per_2'
# 3. 10ê°œ Runì˜ ìš”ì•½ íŒŒì¼ì´ ì €ì¥ë  í´ë”
METRICS_SUMMARY_DIR = 'k10_result_metrics_70per_2' 

os.makedirs(METRICS_SUMMARY_DIR, exist_ok=True)
all_metrics_list = [] # 10ê°œ Runì˜ 'ìµœì¢… ìŠ¤ì¹¼ë¼ ì§€í‘œ'ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸

# --- [ì¶”ê°€ë¨] í‰ê·  ê·¸ë˜í”„ë¥¼ ìœ„í•œ ë³€ìˆ˜ ì„¤ì • ---
# 10ê°œì˜ 'í‰ê°€ ì»¤ë¸Œ' ë°ì´í„°ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
all_p_curves = []  # (Confidence, Precision)
all_r_curves = []  # (Confidence, Recall)
all_f1_curves = [] # (Confidence, F1)
all_pr_curves = [] # (Recall, Precision)

# í‰ê· ì„ ë‚´ê¸° ìœ„í•œ ê³µí†µ x-ì¶• (0ë¶€í„° 1ê¹Œì§€ 101ê°œ ì§€ì )
# np.linspace(start, stop, num_points)
common_confidence_axis = np.linspace(0, 1, 101)
common_recall_axis = np.linspace(0, 1, 101)
# ---

print(f"--- K={K} Fold Evaluation & Prediction Start ---")
print(f"Data source: {DATA_BASE_DIR}")
print(f"Models source: {MODELS_BASE_DIR}")

# --- 3. K=10 (1~10) ë£¨í”„ ì‹¤í–‰ ---
for i in range(1, K + 1):
    run_name = f'run_{i}_test'
    
    print("\n" + "="*60)
    print(f"ğŸš€ [Run {i}/{K}] STARTING: {run_name}")
    print("="*60 + "\n")

    # --- 4. ì´ Runì— í•„ìš”í•œ ê²½ë¡œ ì •ì˜ ---
    model_path = os.path.join(MODELS_BASE_DIR, run_name, 'weights', 'best.pt')
    data_yaml_path = os.path.join(DATA_BASE_DIR, run_name, 'data.yaml')
    predict_source_path = os.path.join(DATA_BASE_DIR, run_name, 'test', 'images')

    # --- 5. íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ---
    if not os.path.exists(model_path):
        print(f"âš ï¸ 'best.pt' ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
    if not os.path.exists(data_yaml_path):
        print(f"âš ï¸ 'data.yaml' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_yaml_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
    if not os.path.exists(predict_source_path):
        print(f"âš ï¸ 'test/images' í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {predict_source_path}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue
        
    # --- 6. ëª¨ë¸ ë¡œë“œ ---
    try:
        model = YOLO(model_path)
        model.to('cpu') # ìš”ì²­í•˜ì‹  ëŒ€ë¡œ CPUë¡œ ì„¤ì •
        print(f"âœ… Model loaded: {model_path}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue

    #####################
    # 7. [ìˆ˜ì •ë¨] ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰ & ì»¤ë¸Œ ë°ì´í„° ì¶”ì¶œ #
    #####################
    print(f"  [1/2] Evaluating 'test' split...")
    try:
        metrics = model.val(
            data=data_yaml_path,
            split='test',        # âœ… 'test' ìŠ¤í”Œë¦¿ì„ ì‚¬ìš©í•˜ë„ë¡ ëª…ì‹œ
            imgsz=416,
            batch=16,
            project=EVAL_PROJECT,
            name=run_name,
            exist_ok=True,
            device='cpu'         # CPUì—ì„œ í‰ê°€ ìˆ˜í–‰
        )

        # --- (A) ìµœì¢… ìŠ¤ì¹¼ë¼(ìˆ«ì) ì§€í‘œ ì¶”ì¶œ ---
        map50 = metrics.box.map50
        map_all = metrics.box.map
        precision = metrics.box.mp
        recall = metrics.box.mr
        
        all_metrics_list.append({
            'run': run_name, 'map50': map50, 'map': map_all,
            'precision': precision, 'recall': recall
        })

        # í‰ê°€ ì§€í‘œ ì¶œë ¥
        print(f"    ğŸ“Š [Metrics for {run_name}]")
        print(f"    mAP@0.5:          {map50:.4f}")
        print(f"    mAP@0.5:0.95:     {map_all:.4f}")

        # --- [ì¶”ê°€ë¨] (B) ì»¤ë¸Œ ì›ì‹œ ë°ì´í„° ì¶”ì¶œ ë° ë³´ê°„(Interpolation) ---
        print("    ... í‰ê°€ ì»¤ë¸Œ(P, R, F1, PR) ì›ì‹œ ë°ì´í„° ì¶”ì¶œ ì¤‘ ...")
        
        # (ì¤‘ìš”!) metrics.curvesê°€ Noneì¸ì§€(ê°ì§€ëœ ê°ì²´ê°€ 0ê°œì¸ì§€) í™•ì¸
        if metrics.curves is None or 'Precision' not in metrics.curves:
            print("    âš ï¸ ì»¤ë¸Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (0 Detections?). ì´ Runì˜ ì»¤ë¸ŒëŠ” í‰ê·  ê³„ì‚°ì—ì„œ ì œì™¸ë©ë‹ˆë‹¤.")
        else:
            # ì»¤ë¸Œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
            p_data = metrics.curves['Precision']
            interp_p = np.interp(common_confidence_axis, p_data[0][::-1], p_data[1][::-1])
            all_p_curves.append(interp_p)

            r_data = metrics.curves['Recall']
            interp_r = np.interp(common_confidence_axis, r_data[0][::-1], r_data[1][::-1])
            all_r_curves.append(interp_r)

            f1_data = metrics.curves['F1']
            interp_f1 = np.interp(common_confidence_axis, f1_data[0][::-1], f1_data[1][::-1])
            all_f1_curves.append(interp_f1)
            
            pr_data = metrics.curves['PR']
            interp_pr = np.interp(common_recall_axis, pr_data[0], pr_data[1])
            all_pr_curves.append(interp_pr)
            print("    ... ì»¤ë¸Œ ë°ì´í„° ì¶”ì¶œ ì„±ê³µ.")

        # (ê¸°ì¡´) ê°œë³„ í‰ê°€ ì§€í‘œ íŒŒì¼ ì €ì¥
        output_dir = os.path.join(METRICS_SUMMARY_DIR, run_name)
        os.makedirs(output_dir, exist_ok=True)
        metrics_path = os.path.join(output_dir, 'metrics.txt')

        with open(metrics_path, 'w') as f:
            f.write(f"[ëª¨ë¸ í‰ê°€ ì§€í‘œ: {run_name}]\n")
            f.write(f"mAP@0.5:          {map50:.4f}\n")
            f.write(f"mAP@0.5:0.95:     {map_all:.4f}\n")
            f.write(f"Precision (mean): {precision:.4f}\n")
            f.write(f"Recall (mean):    {recall:.4f}\n")
        
        print(f"    -> ê°œë³„ metrics.txt ì €ì¥ ì™„ë£Œ: {metrics_path}")

    except Exception as e:
        print(f"âŒ í‰ê°€ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}. ì´ Runì€ ê±´ë„ˆëœë‹ˆë‹¤.")
        continue # ì˜ˆì™¸ ë°œìƒ ì‹œ ë‹¤ìŒ Runìœ¼ë¡œ

    #######################################
    # 8. ì˜ˆì¸¡ ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ (ì‹œê°í™”ìš©) #
    #######################################
    print(f"  [2/2] Predicting on 'test/images'...")
    try:
        results = model.predict(
            source=predict_source_path,
            imgsz=416,
            save=True,
            project=PREDICT_PROJECT,
            name=run_name,
            exist_ok=True,
            batch=16,
            device='cpu'
        )
        print(f"    -> ì˜ˆì¸¡ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ (Project: {PREDICT_PROJECT})")
    
    except Exception as e:
        print(f"âŒ ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
    print(f"âœ… [Run {i}/{K}] FINISHED: {run_name}")


# --- 9. [ìµœì¢… ìš”ì•½] 10ê°œ Runì˜ í‰ê·  ìŠ¤ì¹¼ë¼ ì§€í‘œ ê³„ì‚° ---
print("\n" + "="*60)
print("ğŸ‰ All 10 K-Fold evaluations are complete.")
print(f"ğŸ“Š [K-Fold ìµœì¢… ìŠ¤ì¹¼ë¼ ìš”ì•½ (from {DATA_BASE_DIR})]")
print("="*60 + "\n")

if not all_metrics_list:
    print("âŒ ê³„ì‚°ëœ metricsê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤ì¹¼ë¼ ìš”ì•½ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
else:
    # mAP@0.5 ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥
    all_metrics_list.sort(key=lambda x: x['map50'], reverse=True)
    
    summary_file_path = os.path.join(METRICS_SUMMARY_DIR, '_K10_Final_Summary.txt')
    
    with open(summary_file_path, 'w') as f:
        f.write(f"[K-Fold ìµœì¢… ìš”ì•½: {DATA_BASE_DIR}]\n\n")
        f.write("--- ê°œë³„ Run ì„±ëŠ¥ (mAP@0.5 ê¸°ì¤€ ì •ë ¬) ---\n")
        
        for metrics in all_metrics_list:
            line = f"  {metrics['run']}: mAP@0.5={metrics['map50']:.4f}, mAP@.5:.95={metrics['map']:.4f}, P={metrics['precision']:.4f}, R={metrics['recall']:.4f}\n"
            print(line, end='')
            f.write(line)

        # í‰ê·  ê³„ì‚°
        avg_map50 = sum(m['map50'] for m in all_metrics_list) / len(all_metrics_list)
        avg_map = sum(m['map'] for m in all_metrics_list) / len(all_metrics_list)
        avg_p = sum(m['precision'] for m in all_metrics_list) / len(all_metrics_list)
        avg_r = sum(m['recall'] for m in all_metrics_list) / len(all_metrics_list)
        
        print("-------------------------------------------------")
        print(f"  ğŸ”¥ AVERAGE mAP@0.5:      {avg_map50:.4f}")
        print(f"  ğŸ”¥ AVERAGE mAP@.5:.95: {avg_map:.4f}")
        print(f"  ğŸ”¥ AVERAGE Precision:  {avg_p:.4f}")
        print(f"  ğŸ”¥ AVERAGE Recall:     {avg_r:.4f}")

        f.write("\n--- K-Fold í‰ê·  (N=10) ---\n")
        f.write(f"AVERAGE mAP@0.5:      {avg_map50:.4f}\n")
        f.write(f"AVERAGE mAP@.5:.95: {avg_map:.4f}\n")
        f.write(f"AVERAGE Precision:  {avg_p:.4f}\n")
        f.write(f"AVERAGE Recall:     {avg_r:.4f}\n")
        
    print(f"\nâœ… K-Fold ìµœì¢… ìš”ì•½ íŒŒì¼ ì €ì¥ ì™„ë£Œ!\n   -> {summary_file_path}")

# --- 10. [ì¶”ê°€ë¨] ìµœì¢… í‰ê·  í‰ê°€ ê·¸ë˜í”„ ìƒì„± ---
print("\n" + "="*60)
print(f"ğŸ“Š [K-Fold í‰ê·  í‰ê°€ ê·¸ë˜í”„ ìƒì„± (from {DATA_BASE_DIR})]")
print("="*60 + "\n")

# ì»¤ë¸Œ ë°ì´í„°ê°€ í•˜ë‚˜ë¼ë„ ìˆ˜ì§‘ë˜ì—ˆëŠ”ì§€ í™•ì¸
if not all_pr_curves:
    print("âŒ ê³„ì‚°ëœ ì»¤ë¸Œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
else:
    print(f"  ... {len(all_pr_curves)}ê°œì˜ ìœ íš¨í•œ Run ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í‰ê·  ê·¸ë˜í”„ ìƒì„± ì¤‘ ...")
    
    # 2x2 ì„œë¸Œí”Œë¡¯ ìƒì„±
    fig, axes = plt.subplots(2, 2, figsize=(20, 12))
    
    # --- ê·¸ë˜í”„ 1 (ì¢Œìƒë‹¨): Precision vs Confidence ---
    ax = axes[0, 0]
    avg_p_curve = np.mean(all_p_curves, axis=0) # 10ê°œ ì»¤ë¸Œ í‰ê· 
    ax.plot(common_confidence_axis, avg_p_curve, label='Average P')
    ax.set_title(f'Average Precision vs Confidence Curve (N={len(all_p_curves)})')
    ax.set_xlabel('Confidence')
    ax.set_ylabel('Precision')
    ax.legend()
    ax.grid(True)
    
    # --- ê·¸ë˜í”„ 2 (ìš°ìƒë‹¨): Recall vs Confidence ---
    ax = axes[0, 1]
    avg_r_curve = np.mean(all_r_curves, axis=0) # 10ê°œ ì»¤ë¸Œ í‰ê· 
    ax.plot(common_confidence_axis, avg_r_curve, label='Average R', color='orange')
    ax.set_title(f'Average Recall vs Confidence Curve (N={len(all_r_curves)})')
    ax.set_xlabel('Confidence')
    ax.set_ylabel('Recall')
    ax.legend()
    ax.grid(True)

    # --- ê·¸ë˜í”„ 3 (ì¢Œí•˜ë‹¨): F1 vs Confidence ---
    ax = axes[1, 0]
    avg_f1_curve = np.mean(all_f1_curves, axis=0) # 10ê°œ ì»¤ë¸Œ í‰ê· 
    ax.plot(common_confidence_axis, avg_f1_curve, label='Average F1', color='green')
    ax.set_title(f'Average F1 vs Confidence Curve (N={len(all_f1_curves)})')
    ax.set_xlabel('Confidence')
    ax.set_ylabel('F1 Score')
    ax.legend()
    ax.grid(True)

    # --- ê·¸ë˜í”„ 4 (ìš°í•˜ë‹¨): Precision vs Recall (PR Curve) ---
    ax = axes[1, 1]
    avg_pr_curve = np.mean(all_pr_curves, axis=0) # 10ê°œ ì»¤ë¸Œ í‰ê· 
    ax.plot(common_recall_axis, avg_pr_curve, label='Average PR Curve', color='red')
    ax.set_title(f'Average Precision-Recall (PR) Curve (N={len(all_pr_curves)})')
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.legend()
    ax.grid(True)

    # --- ì „ì²´ ì €ì¥ ---
    fig.suptitle(f'K={K} Fold Average Evaluation Curves - {DATA_BASE_DIR}', fontsize=24, y=1.03)
    plt.tight_layout()
    output_png_path = os.path.join(METRICS_SUMMARY_DIR, f'_K10_Final_Avg_Evaluation_Curves.png')
    
    # 
    # â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸
    # 
    #           âœ…âœ…âœ… ì´ ë¶€ë¶„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤! âœ…âœ…âœ…
    #           'dpi=300' (ê³ í•´ìƒë„) ì˜µì…˜ì„ ì œê±°í–ˆìŠµë‹ˆë‹¤.
    # 
    # â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸â€¼ï¸
    # 
    plt.savefig(output_png_path, bbox_inches='tight') 
    plt.close()

    print(f"âœ… 2x2 í‰ê·  í‰ê°€ ê·¸ë˜í”„ 1ê°œê°€ '{output_png_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")