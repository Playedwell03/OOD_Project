OOD_Project

echo "# OOD_Project" >> README.md
git init
git add README.md
git commit -m "first commit"
git branch -M main
git remote add origin git@github.com:Playedwell03/OOD_Project.git
git push -u origin main

# 프로젝트 진행상황(12.03)
기존 프로젝트의 목적은 "쓰레기 데이터들을 걸러 모델 성능 향상에 기여하자" 였음.

쓰레기? 쓰레기란 무엇인가?에 대하여 생각함.
쓰레기 데이터를 "학습에 방해가 되는 데이터"라고 정의한 뒤, 다음과 같이 세 가지로 분류.
1. 기존 데이터셋과 주제가 전혀 다른 이미지의 데이터셋
2. 바운딩 박스 정보가 잘못 된 데이터
3. 클래스 정보가 잘못 된 데이터

그렇다면 정상 데이터는 무엇인가?
[ROBOFLOW의 traffic sign 데이터셋]
https://universe.roboflow.com/project-fatj9/traffic_sign-fa883/dataset/8

img_size : 416 x 416
데이터 개수 : 4,056(라벨 기준)

주제가 전혀 다른 데이터 : 물고기 데이터

위의 데이터셋에서 쓰레기 데이터가 아닌 것이 정상 데이터가 되겠음.

이후 정의한 각각의 쓰레기 데이터들을 기존 데이터셋을 이용해 만든 뒤, 학습 및 평가 진행함.

쓰레기를 첨가할수록 모델 성능이 떨어지는 것을 확인하였고, 쓰레기 데이터를 정제하기 위한 작업 진행함.
그러나 작업을 진행하다가 현재 정제하는 방식이 결국 사람이 직접 수동으로 코드를 돌려 전처리하는 방식과 다를게 없음을 깨달음.
"모델이 직접 쓰레기 데이터를 거르게 하자" 방향으로 진행.

따라서 쓰레기 데이터를 전처리를 통해 제거해야하는 2, 3 번을 제외한 주제가 전혀 다른 데이터셋인 물고기 데이터로 고정함.
클래스 또한 5개로 일시적 제한함.

쓰레기를 정제할 방법을 찾기위해 쓰레기를 첨가하여 모델을 돌려보던 중, 보임 쓰레기 데이터에 대해 비정상적으로 높은 확률의 예측을 하는것을 발견함.
찾아보니 원인은 활성화 함수인 softmax에 있었음.
모델은 분류를 위해 예측 점수인 logits를 부여하고, 이 때 logits를 확률로 만들기 위해 softmax함수를 이용함.
그러나, softmax는 예측 확률의 도합이 1이 되어야 한다는 한계가 있음. 따라서 쓰레기 데이터가 어느곳에도 분류하기 애매하다면 높은 예측률을 부여하며 무작위 클래스로 분류함.

'그렇다면, 모델이 분류를 할 때 'trash'라는 새로운 클래스를 부여해 분류하기 애매한 데이터들을 유도할 수 있지 않을까?' 라는 의문 제기.

학습에는 5개 클래스를 그대로 사용하되 평가 시 클래스를 늘려 유도하기 위한 방법을 찾던 중, Openmax 함수를 발견.
OpenMax는 각 클래스별 신뢰도 점수인 logits 값을 각 클래스별로 보정하여 ‘unknown’ 이라는 클래스 점수를 추가하는 방법. 
‘unknown’ 클래스 점수는 각 클래스의 정상 데이터 특징 벡터 평균과 테스트 이미지의 특징 벡터 간 거리를 계산하여 산출됨.

이를 통해 기존 5개의 클래스로 학습을 한 모델이 신뢰도를 출력할 때에는 'unknown' 이라는 클래스를 추가하여 쓰레기 데이터를 분류.
학습을 위한 데이터는 정상 데이터로 유지한 채 test 데이터셋에만 쓰레기 데이터인 물고기 데이터 추가.

(정상 데이터)
정답에 대한 신뢰도 통계 (원본 라벨과 예측 라벨이 같음)
총 이미지 수: 19
최대값: 0.9717
최소값: 0.3203
 
오답에 대한 신뢰도 통계 (원본 라벨과 예측 라벨이 다름)
총 이미지 수: 6
최대값: 0.9878
최소값: 0.2773

(쓰레기 데이터)
총 이미지 수: 5
최대값: 0.2015
최소값: 0.1803

예상한대로 쓰레기 데이터들이 'unknown'이라는 클래스에 낮은 신뢰도를 받은 채 분류되기 시작함.
정상 데이터임에도 모델이 잘 예측하지 못한 낮은 신뢰도의 오답 데이터와 쓰레기 데이터 신뢰도 최대값을 고려해 30% 미만의 신뢰도를 가진 데이터를 걸러내도 괜찮겠다는 생각을 함.
데이터 양 늘려 재진행.

Train : 130 x 5 = 650
Valid : 16 x 5 = 80
Test (쓰레기 약 20%) : 16 x 5 + 15 = 95

총 이미지 개수 : 95개
정상 데이터의 개수 : 80개
쓰레기 데이터의 개수 : 15개

[정상 데이터]
신뢰도 30% 이상 : 79 / 80
신뢰도 30% 미만 : 1/ 15

[쓰레기 데이터]
신뢰도 30% 이상 : 1 / 80
신뢰도 30% 미만 : 14 / 15

실험 결과, 쓰레기 데이터가 대부분 걸러짐을 확인함.

이를 기반으로 쓰레기 데이터 및 애매한 데이터들을 거르는 정제작업 진행

[정제 시작 전]
1.교통 표지판 데이터셋 + 쓰레기(물고기)데이터 -> A데이터셋
2.CNN으로 쓰레기를 거르기 때문에 한 라벨 파일에 라벨 반드시 하나만 존재하게 전처리 진행
3.A데이터셋 3등분 -> A1 / A2 / A3

[정제 작업 시작(파인튜닝 x)]
1.A1으로 학습 -> Model_A1
2. Model_A1으로 A2의 쓰레기 거르기(신뢰도 임계치 25%) 걸러진 쓰레기 G_A2.
3. A2 - G_A2 으로 학습 -> Model_A2
4. Model_A2로 A1의 쓰레기 거르기. 걸러진 쓰레기 G_A1
5. A1 + A2 - G_A1 - G_A2 로 학습 -> Model_A12
6. Model_A12로 A3 쓰레기 거르기. 걸러진 쓰레기 G_A3

[최종 테스트]
1.CNN사용으로 인해 하였던 라벨 전처리 복구
2. A와 A - (G_A1 - G_A2 - G_A3)에서 쓰레기가 아닌 고정 테스트 데이터셋 추출
3. YOLOv5 모델로 A 와 A - (G_A1 - G_A2 - G_A3) 학습
4. 모델로 평가 진행 및 결과 비교

그러나, 결과를 비교해보니 모델 성능이 오히려 좋아지는 모습이 보임.

따라서 "쓰레기 데이터가 모델에 악영향을 주는가?" 에 대한 실험 다시 진행.
클래스를 두 개로 더욱 축소하고, 데이터 양을 크게 줄임.
모델의 구조만 따와 사전 가중치 없이 쓰레기 데이터를 첨가하는 방식으로 0%(쓰레기 x), 10%, 30%, 50% 데이터셋 구성.

실험 결과, 모델의 학습이 망가지고 성능이 나빠짐. 이는 데이터 양을 늘려 진행하였을때도 같은 모습을 보임.

클래스와 데이터 양을 모두 복구한 채 동일 실험 진행함.
학습에서는 쓰레기 데이터를 넣지 않은 모델이 가장 안정적인 모습을 보였고, 10%, 30%, 50% 모델이 그 뒤를 이음.
그러나, 평가에서는 결과가 완전히 뒤집혀 30%, 10%, 50%, 0%(쓰레기 x) 모델 순서로 우수한 성능 모습을 보임.

k-fold 교차검증 진행. 그러나 여전히 비슷한 결과를 보임. 쓰레기 비율이 50%나 됨에도 성능 차이가 없는 모습에 실험 방식을 점검해 보았으나 문제 없었음.

그러다 batch size 값이 높으면 모델이 쓰레기 데이터에 더욱 강한 모습을 보인다는 논문을 찾게 되어 간단히만 보고 batch size를 16에서 4로 낮춘 채 추가실험 진행.
추가로 실험해 본 결과, 쓰레기 10% 모델과 쓰레기를 섞지 않은 모델은 비슷한 성능을 보였으나 쓰레기를 더욱 섞을수록 성능이 눈에 띄게 감소하는 모습을 보임.
교차 검증하지 않은 결과라 확실하진 않지만 쓰레기를 섞었음에도 성능이 유지되는 이유 중 하나가 batch size인 것으로 추정.

‘클래스 간의 쓰레기 데이터 종류가 겹침에 대한 영향은 없을까?’ 의문이 들어 각 클래스마다 일관성있는 쓰레기 데이터를 첨가하면 모델이 무너질 것이라 예상하고 아래 실험도 진행해보면 어떨까 생각.
ex) 
- 1번 클래스에 정상 데이터 50%, 물고기 데이터(쓰레기) 50%
- 2번 클래스에 정상 데이터 50%, 알약 데이터(쓰레기) 50%
- 3번 클래스에 정상 데이터 50%, 고양이 데이터(쓰레기) 50%
